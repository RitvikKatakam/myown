{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e68471-ba4b-4f25-a040-8825d1716443",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# imports\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499857be-92e0-4d37-a30a-9beecccb0ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288025a-783a-4a9d-96ab-a55634a0bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d54d6b-663d-4482-9079-1b4ae3a1899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "# If you're not familiar with Classes, check out the \"Intermediate Python\" notebook\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba207b-5786-443a-9de6-e8d17e062c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Adi = Website(\"https://www.ncbi.nlm.nih.gov/books/NBK526128/\")\n",
    "print(Adi.title)\n",
    "print(Adi.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3bbe65-e9e6-43fc-a122-ba5eb8deaa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef7899-8e33-4efd-bf2b-90097871e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46cb69-3584-4f96-a8ca-2ad519f723cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(user_prompt_for(Adi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b91147-ac04-440f-9ea3-9b4b2c4be6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a snarky assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d8af0-cf3f-4377-9d7b-ded479db2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54412019-0ea3-454e-ba05-722cf8458e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To give you a preview -- calling OpenAI with system and user messages:\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe057a-0514-4a4a-9890-3ae4182cce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae4113-1c83-4272-80ef-7858c8ae36fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages_for(Adi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb5ea7-3506-4d91-9324-de6f60ec5c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d02fc-91cf-4ba5-b284-075063771acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(\"https://www.ncbi.nlm.nih.gov/books/NBK526128/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a7ac95-3195-42fd-982e-cf5657268308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display the above summary nicely in the Jupyter output, using markdown\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4579757-6805-41bb-8413-7afd436c97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://www.ncbi.nlm.nih.gov/books/NBK526128/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb21320-ac50-4ed5-8385-76b01c33b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR SOME SITES WHICH USES JAVA SCRIPT , IT CANT SUMMARIZE DUE TO EXTRACTION PROBLEMS . TO ENCOUNTER THIS WE USE SELLENIUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c6c6b-62a1-4347-8595-006556f7c7ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install openai selenium webdriver-manager python-dotenv beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3956e9c8-1c45-4232-ae11-067fe9d3cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from openai import OpenAI\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ✅ Load OpenAI API Key\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"OpenAI API key missing!\")\n",
    "else:\n",
    "    print(\"API key loaded successfully.\")\n",
    "\n",
    "# ✅ Initialize OpenAI Client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# ✅ Function to fetch page content using Selenium\n",
    "def get_page_content(url):\n",
    "    options = Options()\n",
    "    options.headless = True  # ✅ Headless browser mode\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Optional: Wait until page loads\n",
    "        driver.implicitly_wait(5)\n",
    "        page_source = driver.page_source\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return page_source\n",
    "\n",
    "# ✅ Function to extract text from HTML\n",
    "def extract_text_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Remove script & style elements\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.decompose()\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return text.strip()\n",
    "\n",
    "# ✅ Function to summarize using OpenAI\n",
    "def summarize_text(text, max_tokens=300):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the following webpage content:\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# ✅ Main Workflow\n",
    "def summarize_website(url):\n",
    "    html_content = get_page_content(url)\n",
    "    page_text = extract_text_from_html(html_content)\n",
    "    if len(page_text) > 4000:  \n",
    "        page_text = page_text[:4000]\n",
    "    summary = summarize_text(page_text)\n",
    "    display(Markdown(f\"### Summary of [{url}]({url})\\n\\n{summary}\"))\n",
    "\n",
    "# ✅ Example Usage\n",
    "summarize_website(\"https://www.bbc.com/news\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877cc18-5f4c-4563-88da-68a2abfd48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_website(\"https://openai.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5c980-19be-498a-9d53-9b10d412a323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc9409-0880-4059-aa9d-e71a33d469c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abe235-59f7-4a41-bb1c-6d0eff65cf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.3-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: openai in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (1.95.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading pymupdf-1.26.3-cp39-abi3-win_amd64.whl (18.7 MB)\n",
      "   ---------------------------------------- 0.0/18.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 3.1/18.7 MB 23.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 4.5/18.7 MB 12.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 7.9/18.7 MB 14.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 8.9/18.7 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 10.0/18.7 MB 10.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 13.1/18.7 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 14.4/18.7 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.4/18.7 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.7/18.7 MB 11.4 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.3\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF openai\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2436d89-45a8-44d4-9907-93938c952a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "print(len(pdf_text))\n",
    "print(pdf_text[:1000])  # Print first 1000 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2218e371-ada5-4576-8eda-681277668d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\dell\\anaconda3\\envs\\llms\\lib\\site-packages (from pytesseract) (24.2)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytesseract, pdf2image\n",
      "\n",
      "   ---------------------------------------- 2/2 [pdf2image]\n",
      "\n",
      "Successfully installed pdf2image-1.17.0 pytesseract-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract pdf2image pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a268a13-8bee-41bf-854a-84e815969ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def extract_text_with_ocr(pdf_path):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    text = \"\"\n",
    "    for img in images:\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546132a3-0cfe-4139-9514-a0e2124be416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No text found in PDF, using OCR...\n",
      "Summarizing chunk 1 of 3...\n",
      "Summarizing chunk 2 of 3...\n",
      "Summarizing chunk 3 of 3...\n",
      "Summarizing combined summaries...\n",
      "\n",
      "Final Summary:\n",
      "\n",
      "The document is an Aadhaar card registration or enrollment letter from the Unique Identification Authority of India (UIDAI). It contains personal details of an individual, including their name, address in Telangana, Aadhar number, date of birth, and contact information. Key highlights include:\n",
      "\n",
      "1. **Aadhaar as Identity Proof**: It is emphasized that Aadhaar serves as proof of identity but not as confirmation of citizenship or date of birth. Users are encouraged to verify their information through online methods or QR code scanning.\n",
      "\n",
      "2. **Documentation Updates**: Individuals should update their identity and address supporting documents every ten years.\n",
      "\n",
      "3. **Aadhaar Benefits**: It allows access to a variety of government and non-government services.\n",
      "\n",
      "4. **Security Practices**: Users should keep their contact information current and are advised to use the mAadhaar app for managing Aadhaar services securely. \n",
      "\n",
      "5. **Consent for Processing**: Any entity requesting Aadhaar details must have the individual's explicit consent.\n",
      "\n",
      "The letter underscores the significance of Aadhaar for identity verification and outlines steps for maintaining accurate records. It also mentions potential formatting issues within the document. The document includes contact information for UIDAI for further support.\n"
     ]
    }
   ],
   "source": [
    "# 📄 PDF Summarizer with Chunking Support using OpenAI API + OCR fallback\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import openai\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "# Set Tesseract OCR path manually (no need to set system PATH)\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "\n",
    "# Set your OpenAI API Key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# OCR fallback for image-based PDFs\n",
    "def extract_text_with_ocr(pdf_path):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    text = \"\"\n",
    "    for img in images:\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "# Function to chunk text based on approximate token size\n",
    "def chunk_text(text, max_tokens=3000):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        current_length += len(word) + 1  # +1 for space\n",
    "        if current_length > max_tokens:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(word)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to summarize text using OpenAI API\n",
    "def summarize_text(text, model=\"gpt-4o-mini\"):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the following PDF content.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Function to summarize large text by splitting and summarizing each chunk\n",
    "def summarize_large_text(text, model=\"gpt-4o-mini\"):\n",
    "    chunks = chunk_text(text)\n",
    "    summaries = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Summarizing chunk {i+1} of {len(chunks)}...\")\n",
    "        summary = summarize_text(chunk, model=model)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    print(\"Summarizing combined summaries...\")\n",
    "    final_summary = summarize_text(' '.join(summaries), model=model)\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = r\"C:\\Users\\DELL\\Downloads\\aadhar.pdf\"\n",
    "\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "if not pdf_text.strip():\n",
    "    print(\"No text found in PDF, using OCR...\")\n",
    "    pdf_text = extract_text_with_ocr(pdf_path)\n",
    "\n",
    "if not pdf_text.strip():\n",
    "    print(\"Still empty after OCR. Please check your PDF file.\")\n",
    "else:\n",
    "    final_summary = summarize_large_text(pdf_text)\n",
    "    print(\"\\nFinal Summary:\\n\")\n",
    "    print(final_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
